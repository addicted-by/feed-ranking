{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "TRAIN_DATA_PATH = 'data/mind_large_train/news.tsv'\n",
    "TEST_DATA_PATH = 'data/mind_large_dev/news.tsv'\n",
    "assert os.path.exists(TRAIN_DATA_PATH)\n",
    "assert os.path.exists(TEST_DATA_PATH)\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_csv(TRAIN_DATA_PATH, sep='\\t', names=['news_id', 'category', 'subcategory', 'title', 'abstract', 'url', 'title_entities', 'abstract_entities'])\n",
    "test_df =  pd.read_csv(TEST_DATA_PATH, sep='\\t', names=['news_id', 'category', 'subcategory', 'title', 'abstract', 'url', 'title_entities', 'abstract_entities'])\n",
    "train_df_clf = train_df[['news_id', 'category', 'title', 'abstract']]\n",
    "\n",
    "train_df_clf['abstract'].fillna('', inplace=True)\n",
    "train_df_clf['text'] = train_df_clf[\"title\"] + train_df_clf['abstract']\n",
    "assert train_df_clf.text.isna().sum() == 0\n",
    "\n",
    "test_df_clf = test_df[['news_id', 'category', 'title', 'abstract']]\n",
    "test_df_clf['abstract'].fillna('', inplace=True)\n",
    "test_df_clf['title'].fillna('', inplace=True)\n",
    "test_df_clf['text'] = test_df_clf[\"title\"] + test_df_clf['abstract']\n",
    "\n",
    "\n",
    "test_df_clf = test_df_clf.drop(columns=['title', 'abstract']).set_index('news_id')\n",
    "train_df_clf = train_df_clf.drop(columns=['title', 'abstract']).set_index('news_id')\n",
    "\n",
    "train_df_clf.drop_duplicates(inplace=True)\n",
    "test_df_clf.drop_duplicates(inplace=True)\n",
    "\n",
    "\n",
    "df_clf = pd.concat([train_df_clf,test_df_clf]).drop_duplicates()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# with open('embs.json', 'w', encoding='utf-8') as f:\n",
    "#     json.dump(embs, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('embs.json', 'r', encoding='utf-8') as f:\n",
    "    check = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model_emb = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"it5/it5-base-news-summarization\")\n",
    "# model = AutoModelForSeq2SeqLM.from_pretrained(\"it5/it5-base-news-summarization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clf = df_clf.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = set(map(str, df_clf.index)) - set(list(check.keys()))\n",
    "assert len(diff) < 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for value in diff:\n",
    "    check[value] = model_emb.encode(df_clf.loc[int(value)]['text']).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert set(map(str, df_clf.index)) - set(list(check.keys())) == set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clf['summarization_emb:float_seq'] = list(check.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.plotting import figure, show\n",
    "from bokeh.models import ColumnDataSource, HoverTool\n",
    "from bokeh.io import output_notebook\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=2, random_state=0xDEAD)\n",
    "embeddings_2d = tsne.fit_transform(df_clf['summarization_emb:float_seq'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clf['embedding_2D_0'] = embeddings_2d[:, 0]\n",
    "df_clf['embedding_2D_1'] = embeddings_2d[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.palettes import Category20\n",
    "unique_genres = df_clf['category'].unique()\n",
    "num_genres = len(unique_genres)\n",
    "\n",
    "# Define the colormap\n",
    "colormap = Category20[num_genres]\n",
    "genre_colors = {genre: colormap[i % num_genres] for i, genre in enumerate(unique_genres)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.transform import factor_cmap\n",
    "source = ColumnDataSource(df_clf.head(20000))\n",
    "\n",
    "# Create the figure and add glyphs\n",
    "p = figure(title='Summarization Embeddings Visualization', x_axis_label='Embedding Dimension 1', y_axis_label='Embedding Dimension 2')\n",
    "p.scatter('embedding_2D_0', 'embedding_2D_1', source=source, color=factor_cmap('category', palette=colormap, factors=unique_genres))#{'field': 'category', 'transform': genre_colors})\n",
    "hover = HoverTool(tooltips=[('Text', '@text'), ('Category', '@category')])\n",
    "p.add_tools(hover)\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clf[\"summarization_emb:float_seq\"] = df_clf[\"summarization_emb:float_seq\"].apply(lambda row: ' '.join([str(x) for x in row]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clf.drop(columns=[\"category\", \"text\", \"embedding_2D_0\", \"embedding_2D_1\"], inplace=True)\n",
    "df_clf = df_clf.rename({\"news_id\" : \"sid:token\"}, axis=1)\n",
    "df_clf.to_csv('mind_large.sumemb', index=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clf.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
